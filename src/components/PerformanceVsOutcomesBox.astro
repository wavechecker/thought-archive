---
---

<section class="explainer-box">
  <h3>ğŸ“Š Performance Metrics vs Clinical Outcomes</h3>

  <p>
    Many AI studies report strong <strong>performance metrics</strong>.  
    These measure how well an algorithm detects patterns.
  </p>

  <ul>
    <li><strong>Sensitivity</strong> â€“ How often the model correctly identifies disease</li>
    <li><strong>Specificity</strong> â€“ How often it correctly rules disease out</li>
    <li><strong>Accuracy</strong> â€“ Overall correct classifications</li>
    <li><strong>Area Under the Curve (AUC)</strong> â€“ Overall diagnostic discrimination ability</li>
  </ul>

  <p>
    These metrics are important â€” but they do <em>not automatically demonstrate</em> clinical benefit.
  </p>

  <hr />

  <p>
    <strong>Clinical outcomes</strong> measure what ultimately matters to patients:
  </p>

  <ul>
    <li>Reduced mortality</li>
    <li>Fewer complications</li>
    <li>Shorter hospital stays</li>
    <li>Improved quality of life</li>
    <li>Lower unnecessary interventions</li>
  </ul>

  <p>
    An AI tool may detect disease with high accuracy yet fail to improve outcomes
    if it increases false positives, overdiagnosis, or inappropriate treatment.
  </p>

  <p>
    The central question is not just:
  </p>

  <blockquote>
    â€œDoes the algorithm detect patterns well?â€
  </blockquote>

  <p>
    But rather:
  </p>

  <blockquote>
    â€œDoes its use improve patient outcomes safely and consistently?â€
  </blockquote>
</section>

<style>
  .explainer-box {
    border-left: 4px solid #7c3aed;
    background: #faf5ff;
    padding: 1.25rem;
    margin: 2rem 0;
    border-radius: 6px;
  }

  .explainer-box h3 {
    margin-top: 0;
  }

  .explainer-box ul {
    margin-left: 1.25rem;
  }

  .explainer-box blockquote {
    margin: 0.75rem 0;
    padding-left: 1rem;
    border-left: 3px solid #c4b5fd;
    font-style: italic;
  }

  .explainer-box hr {
    border: none;
    border-top: 1px solid #e9d5ff;
    margin: 1rem 0;
  }
</style>
